{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a292a04-ef77-4ca3-b084-6e74a9930d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images on the page.\n",
      "Downloading image: https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png\n",
      "Saved: images\\Wikipedia-logo-v2.png\n",
      "Downloading image: https://upload.wikimedia.org/wikipedia/donate/1/14/Wikimedia_Foundation_logo_-_wordmark.svg\n",
      "Saved: images\\Wikimedia_Foundation_logo_-_wordmark.svg\n",
      "Image scraping completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "\n",
    "# Function to scrape images from a webpage\n",
    "def scrape_images_from_webpage(url, output_dir=\"images\"):\n",
    "    \"\"\"\n",
    "    Scrape images from the given webpage and save them locally.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The webpage URL.\n",
    "    output_dir (str): Directory to save images (default is \"images\").\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all <img> tags\n",
    "        img_tags = soup.find_all('img')\n",
    "        print(f\"Found {len(img_tags)} images on the page.\")\n",
    "\n",
    "        # Download each image\n",
    "        for img_tag in img_tags:\n",
    "            # Get the image URL\n",
    "            img_url = img_tag.get('src')\n",
    "            if not img_url:\n",
    "                continue  # Skip if no URL found\n",
    "            \n",
    "            # Convert relative URLs to absolute URLs\n",
    "            img_url = urljoin(url, img_url)\n",
    "            print(f\"Downloading image: {img_url}\")\n",
    "\n",
    "            # Retrieve and save the image\n",
    "            img_data = requests.get(img_url).content\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(output_dir, img_name)\n",
    "            \n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_data)\n",
    "            \n",
    "            print(f\"Saved: {img_path}\")\n",
    "        \n",
    "        print(\"Image scraping completed.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # URL of the webpage to scrape\n",
    "    webpage_url = \"https://www.wikipedia.org/\"\n",
    "    scrape_images_from_webpage(webpage_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b6b35-a143-4cc8-9dd1-312f35bbd653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
